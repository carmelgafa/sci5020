\relax 
\citation{mahmud2013will}
\@writefile{toc}{\contentsline {section}{\numberline {1}Question 1}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Give reference to a publication in which the exponential distribution has been used in practise. Explain the context in which this distribution has been used in this publication.}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}State the mean and the variance of $X$.}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3} Derive the moment estimator of $\lambda $.}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Use the second moment to obtain another estimator of $\lambda $}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Comment on the unbiasedness and consistency of the moment estimator for $\lambda $ derived in Q1iii. State any assumption/s that need to be made to check for unbiasedness and consistency.}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unbiasedness:}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Consistency:}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Use R software to generate 1000 data points from an exponential distributed random variable using any admissible parameter value for $\lambda $}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Exponential distributed random variable; histogram of 1000 generated points and theoretical distribution}}{6}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:img-1-6-1}{{1}{6}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1}Write down the log-likelihood function for this exponentially distributed random variable.}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.2}Evaluate the log-likelihood function for the generated data as a function of $\lambda $, and plot the resulting log-likelihood function against different values of $\lambda $. Present the plot together with the answers.}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Log-likelihood plot for an exponential distribution with varying $\lambda $}}{8}{}\protected@file@percent }
\newlabel{fig:img-1-6-2}{{2}{8}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.3}Using the plot or otherwise, which estimate for $\lambda $ is the MLE? Give a reason for your answer.}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Question 2}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}State the type of estimator that needs to be used in this situation and mention two properties of this estimator.}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Derive the equations that need to be solved to obtain the required estimates.}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3} Generate or source a dataset for which you would use this type of model and fit the model to the data. Present a plot of the fitted model to the data. Comment on the goodness of fit of the model to the data. The R code used for this question should be presented together with the answers. Proper referencing should be provided in the text if the data is sourced.}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Question 3}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Derive the maximum likelihood estimator/s of the parameters of the chosen distribution.}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Find the Cramer-Rao lower bound for the maximum likelihood estimator/s obtained in Q3i).}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Does/do the ML estimator/s obtained in Q3i) attain the Cramer-Rao lower bound? Give a reason for your answer. }{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Is/are the ML estimator/s obtained in Q3i) a sufficient statistic for the population parameter/s? Give a reason for your answer.}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Why are maximum likelihood estimators considered to be desirable estimators? Mention one situation where a maximum likelihood estimator might not be optimal. }{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Question 4 - Jackknife and bootstrap}{10}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}{\ignorespaces Non linear regression code in R}}{11}{}\protected@file@percent }
\newlabel{lst:nls}{{1}{11}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Nonlinear regression fit: observed vs. predicted values}}{12}{}\protected@file@percent }
\newlabel{fig:img-4-1}{{3}{12}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Construct a computer code in R to find the Jackknife and Bootstrap estimators of $a$ and $b$. In the case of Jackknife, section randomly the sampling into 5 partitions of size 10. In the case of Bootstrap, generate 1000 samples of size 100 with replacement. }{12}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}{\ignorespaces Jackknife resampling code in R}}{14}{}\protected@file@percent }
\newlabel{lst:jk}{{2}{14}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Nonlinear regression fit: observed vs. predicted values including Jackknife predictions}}{15}{}\protected@file@percent }
\newlabel{fig:img-4-1-1}{{4}{15}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}{\ignorespaces Bootstrap resampling code in R}}{17}{}\protected@file@percent }
\newlabel{lst:bs}{{3}{17}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Nonlinear regression fit: observed vs. predicted values including Jackknife and Bootstrap predictions}}{18}{}\protected@file@percent }
\newlabel{fig:img-4-1-2}{{5}{18}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}For the Jackknife estimator, find a 95\% confidence interval using the normal distribution and the t-distribution. For the Bootstrap estimator, find a 95\% normal, t and empirical confidence intervals.}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Question 5 â€“ The EM Algorithm }{18}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Confidence Intervals for Parameters $a$ and $b$ Using Jackknife and Bootstrap Methods}}{19}{}\protected@file@percent }
\newlabel{tab:ci_results}{{1}{19}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Simulate 1000 readings from a mixture Gaussian distribution with 3 or more Gaussians.}{19}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}{\ignorespaces Gaussian mixture sample generation code in R}}{21}{}\protected@file@percent }
\newlabel{lst:gausmix}{{4}{21}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5}{\ignorespaces Generation of data for this question}}{22}{}\protected@file@percent }
\newlabel{lst:gem-gausmix}{{5}{22}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Distribution of generated data}}{22}{}\protected@file@percent }
\newlabel{fig:img-5-1}{{6}{22}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Determine initial values $\mu _1^{(0)}, \dots  , \mu _K^{(0)}, \sigma _1^{(0)}, \dots  , \sigma _K^{(0)}, \pi _1^{(0)}, \dots  , \pi _K^{(0)}$ using a K-means clustering approach or otherwise. }{23}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6}{\ignorespaces k-means algorithm to compute the initial values of the mixed Gaussian model parameters}}{24}{}\protected@file@percent }
\newlabel{lst:kmeans}{{6}{24}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Estimated parameters for the initial values of Gaussian mixture model}}{25}{}\protected@file@percent }
\newlabel{tab:gmm_parameters}{{2}{25}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Run the EM algorithm for a number of iterations. Print $\mu _1^{(j)}, \dots  , \mu _k^{(j)}, \sigma _1^{(j)}, \dots  , \sigma _k^{(j)}, \pi _1^{(j)}, \dots  , \pi _k^{(j)}$ for each iteration, and also the log-likelihood, which is given by: $ \DOTSB \sum@ \slimits@ _{n=1}^{N} \ln \left ( \DOTSB \sum@ \slimits@ _{l=1}^{K} \phi (x_i | \mu _l^{(j)}, \sigma _l^{(j)}) \right ). $ Determine when to stop the EM algorithm, either via a maximum number of iterations or through a convergence criterion. However, ensure that the EM algorithm has converged. Plot the trajectory of the estimates and the likelihood by iteration to illustrate this. }{25}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {7}{\ignorespaces Estimation step code in R}}{27}{}\protected@file@percent }
\newlabel{lst:em-est}{{7}{27}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {8}{\ignorespaces Maximization step code in R}}{28}{}\protected@file@percent }
\newlabel{lst:em-max}{{8}{28}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {9}{\ignorespaces Log-likelihood code in R}}{29}{}\protected@file@percent }
\newlabel{lst:em-ll}{{9}{29}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {10}{\ignorespaces Execution code for this question}}{30}{}\protected@file@percent }
\newlabel{lst:em-exec}{{10}{30}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces EM iterations results table}}{31}{}\protected@file@percent }
\newlabel{tab:gmm_iterations_3dp}{{3}{31}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Log-likelihood vs iterations for the execution of the EM algorithm until convergence}}{32}{}\protected@file@percent }
\newlabel{fig:img-5-3}{{7}{32}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Give the final estimated $\mu _k$'s, $\sigma _k$'s and $\pi _k$'s. How do these compare with the coefficients you chose in the simulation? }{32}{}\protected@file@percent }
\bibdata{references}
\bibcite{mahmud2013will}{1}
\bibstyle{plain}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Chosen vs. estimated parameters}}{33}{}\protected@file@percent }
\newlabel{tab:gmm_chosen_vs_estimated_full}{{4}{33}{}{}{}}
\gdef \@abspage@last{33}
